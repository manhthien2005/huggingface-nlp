{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhVKrmbYq3_y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XygOJhPpq6R3"
      },
      "source": [
        "# Training a new tokenizer from an old one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpxehh1kq6R4"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn8sHTOMq6R4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-5O9N18q6R4"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ht1j9xtq6R4"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juW0JKuHq6R4"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmZTbCbiq6R4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu_yzZMDq6R5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4N5unlXq6R5",
        "outputId": "83b19963-cbd6-4b2c-e448-078ad8b1c44c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', \n",
              "      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', \n",
              "      'func_code_url'\n",
              "    ],\n",
              "    num_rows: 412178\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-k0F7lq6R5"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcK_n9Gyq6R5"
      },
      "outputs": [],
      "source": [
        "# Don't uncomment the following line unless your dataset is small!\n",
        "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sRv0IZ0q6R5"
      },
      "outputs": [],
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0ttkh-uq6R5",
        "outputId": "558eb651-180a-48fa-bf9d-60f97d336446"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
              "[]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen = (i for i in range(10))\n",
        "print(list(gen))\n",
        "print(list(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvOu3zMwq6R5"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz6bBrReq6R6"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5n81_W-q6R6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLrVmZnmq6R6",
        "outputId": "14c68e5d-7c13-47ea-db4e-12a55e4c5fca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo',\n",
              " 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokens = old_tokenizer.tokenize(example)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pa1A-7Lq6R7"
      },
      "outputs": [],
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP0xlOxlq6R7",
        "outputId": "e55996e4-f67e-4b84-a446-db94bf90ff59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',\n",
              " 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(example)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv8xSjBzq6R7",
        "outputId": "67f5aeb6-9b01-457a-beb3-3657c477fee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27\n",
              "36"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(tokens))\n",
        "print(len(old_tokenizer.tokenize(example)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L22hMQPq6R7",
        "outputId": "a7d29be4-a6a6-4dd8-cfd2-9aff86da9396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',\n",
              " 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',\n",
              " 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',\n",
              " 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',\n",
              " 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "tokenizer.tokenize(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxpbGaJPq6R7"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjLQjC8qq6R7"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUfYhHqOq6R7"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Aydz1Zq6R7"
      },
      "outputs": [],
      "source": [
        "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "721aXg75rNYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1HjWiQdrJBQ"
      },
      "source": [
        "# Fast tokenizers' special powers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2u1JOzkrJBR"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ6AXCCprJBR"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzMebSOErJBS",
        "outputId": "e1f5961f-6013-4cbe-c5b5-b2c4898568ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'transformers.tokenization_utils_base.BatchEncoding'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "encoding = tokenizer(example)\n",
        "print(type(encoding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSC5whYfrJBS",
        "outputId": "b1ed3656-ee49-425c-f23f-70cd3f5f51d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQmfBf3grJBS",
        "outputId": "704680ae-81a9-4f94-87ca-e9230e8f7bd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciodhIClrJBS",
        "outputId": "dd9bfc21-91ba-4d6e-f501-cfff2d82ab43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',\n",
              " 'Brooklyn', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEMOovVmrJBT",
        "outputId": "d769245c-ca1e-4f79-b257-fc29a12d7692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.word_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHwJMaCBrJBT",
        "outputId": "d6f91a62-68d1-4626-d3a4-0459d6f25ac6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sylvain"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start, end = encoding.word_to_chars(3)\n",
        "example[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEjCsALdrJBT",
        "outputId": "baa36cbc-c00e-4148-8fb3-dc25925d40f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG9ULWHYrJBT",
        "outputId": "ac974ee8-254a-4f50-fda7-4568c1dd9915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},\n",
              " {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n",
              " {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5KBlN1GrJBT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "inputs = tokenizer(example, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAZVtmyJrJBT",
        "outputId": "a70a76ed-07e5-4afa-d0cc-0549fb5cabe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 19])\n",
              "torch.Size([1, 19, 9])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(inputs[\"input_ids\"].shape)\n",
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0gTR6mWrJBT",
        "outputId": "99b22230-daff-4ecf-aac1-6a5854131b5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
        "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cnvz3zzrJBT",
        "outputId": "e1826fdf-96dd-430c-a522-c6cdd11a274b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-MISC',\n",
              " 2: 'I-MISC',\n",
              " 3: 'B-PER',\n",
              " 4: 'I-PER',\n",
              " 5: 'B-ORG',\n",
              " 6: 'I-ORG',\n",
              " 7: 'B-LOC',\n",
              " 8: 'I-LOC'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KmNu_M8rJBT",
        "outputId": "1fd447a8-ce2a-4f02-d526-96244963606e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = []\n",
        "tokens = inputs.tokens()\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        results.append(\n",
        "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
        "        )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1cT9TIRrJBT",
        "outputId": "1dc48e7a-b396-4ddc-cede-052311503ee7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),\n",
              " (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "inputs_with_offsets[\"offset_mapping\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ocLtFRArJBT",
        "outputId": "4e20e3a8-4632-4dcd-d3f6-6b484f5666f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "yl"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[12:14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NWkNVT5rJBT",
        "outputId": "45ea652d-4fab-401b-a863-bda9a5d3c5d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        start, end = offsets[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity\": label,\n",
        "                \"score\": probabilities[idx][pred],\n",
        "                \"word\": tokens[idx],\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqMWGe36rJBU",
        "outputId": "0862b94c-b9ec-409e-888f-663721a55c09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hugging Face"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[33:45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtLWAa-krJBU",
        "outputId": "dadd0e17-50ec-4ee2-8e84-f7e42daabc1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},\n",
              " {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n",
              " {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "idx = 0\n",
        "while idx < len(predictions):\n",
        "    pred = predictions[idx]\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        # Remove the B- or I-\n",
        "        label = label[2:]\n",
        "        start, _ = offsets[idx]\n",
        "\n",
        "        # Grab all the tokens labeled with I-label\n",
        "        all_scores = []\n",
        "        while (\n",
        "            idx < len(predictions)\n",
        "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
        "        ):\n",
        "            all_scores.append(probabilities[idx][pred])\n",
        "            _, end = offsets[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
        "        score = np.mean(all_scores).item()\n",
        "        word = example[start:end]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity_group\": label,\n",
        "                \"score\": score,\n",
        "                \"word\": word,\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "    idx += 1\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BOSxUcvhrRug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y06wREP2rPpu"
      },
      "source": [
        "# Fast tokenizers in the QA pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meS6gDEIrPpu"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Ux8kUMrPpu"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-iVPKA1rPpv",
        "outputId": "86742ff4-4bdb-4a5c-eb0e-f8d11bbd293d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.97773,\n",
              " 'start': 78,\n",
              " 'end': 105,\n",
              " 'answer': 'Jax, PyTorch and TensorFlow'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "context = \"\"\"\n",
        "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRHA9XBfrPpv",
        "outputId": "f8494d4f-e0f6-4e05-d887-56de5692da38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.97149,\n",
              " 'start': 1892,\n",
              " 'end': 1919,\n",
              " 'answer': 'Jax, PyTorch and TensorFlow'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "long_context = \"\"\"\n",
        "🤗 Transformers: State of the Art NLP\n",
        "\n",
        "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
        "question answering, summarization, translation, text generation and more in over 100 languages.\n",
        "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
        "\n",
        "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
        "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
        "can be modified to enable quick research experiments.\n",
        "\n",
        "Why should I use transformers?\n",
        "\n",
        "1. Easy-to-use state-of-the-art models:\n",
        "  - High performance on NLU and NLG tasks.\n",
        "  - Low barrier to entry for educators and practitioners.\n",
        "  - Few user-facing abstractions with just three classes to learn.\n",
        "  - A unified API for using all our pretrained models.\n",
        "  - Lower compute costs, smaller carbon footprint:\n",
        "\n",
        "2. Researchers can share trained models instead of always retraining.\n",
        "  - Practitioners can reduce compute time and production costs.\n",
        "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
        "\n",
        "3. Choose the right framework for every part of a model's lifetime:\n",
        "  - Train state-of-the-art models in 3 lines of code.\n",
        "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
        "  - Seamlessly pick the right framework for training, evaluation and production.\n",
        "\n",
        "4. Easily customize a model or an example to your needs:\n",
        "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
        "  - Model internals are exposed as consistently as possible.\n",
        "  - Model files can be used independently of the library for quick experiments.\n",
        "\n",
        "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question_answerer(question=question, context=long_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuIqqORNrPpv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM6vho00rPpv",
        "outputId": "a2f9096d-b623-427a-f15e-e9b38bb298b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 66]) torch.Size([1, 66])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEsE2orfrPpw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "mask = torch.tensor(mask)[None]\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdjpuZS3rPpw"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzzVzyferPpw"
      },
      "outputs": [],
      "source": [
        "scores = start_probabilities[:, None] * end_probabilities[None, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hqI8pIKrPpw"
      },
      "outputs": [],
      "source": [
        "scores = torch.triu(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f36VKkimrPpw",
        "outputId": "869a80fd-4679-46e2-d3ac-e4a188922511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.97773"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_index = scores.argmax().item()\n",
        "start_index = max_index // scores.shape[1]\n",
        "end_index = max_index % scores.shape[1]\n",
        "print(scores[start_index, end_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yytP8X9krPpw"
      },
      "outputs": [],
      "source": [
        "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "start_char, _ = offsets[start_index]\n",
        "_, end_char = offsets[end_index]\n",
        "answer = context[start_char:end_char]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDuYekBdrPpw",
        "outputId": "e34a105c-8da6-430d-e9b7-7ef1963fcdf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': 'Jax, PyTorch and TensorFlow',\n",
              " 'start': 78,\n",
              " 'end': 105,\n",
              " 'score': 0.97773}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = {\n",
        "    \"answer\": answer,\n",
        "    \"start\": start_char,\n",
        "    \"end\": end_char,\n",
        "    \"score\": scores[start_index, end_index],\n",
        "}\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ufszDS0rPpw",
        "outputId": "8bea0f03-b3d8-4f98-b2dd-14ba38f9c1a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "461"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(question, long_context)\n",
        "print(len(inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4ot3P4jrPpw",
        "outputId": "31de14d8-c017-4668-fdc9-611b1a3ad715"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\"\"\n",
              "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP\n",
              "\n",
              "[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
              "question answering, summarization, translation, text generation and more in over 100 languages.\n",
              "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
              "\n",
              "[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
              "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
              "can be modified to enable quick research experiments.\n",
              "\n",
              "Why should I use transformers?\n",
              "\n",
              "1. Easy-to-use state-of-the-art models:\n",
              "  - High performance on NLU and NLG tasks.\n",
              "  - Low barrier to entry for educators and practitioners.\n",
              "  - Few user-facing abstractions with just three classes to learn.\n",
              "  - A unified API for using all our pretrained models.\n",
              "  - Lower compute costs, smaller carbon footprint:\n",
              "\n",
              "2. Researchers can share trained models instead of always retraining.\n",
              "  - Practitioners can reduce compute time and production costs.\n",
              "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
              "\n",
              "3. Choose the right framework for every part of a model's lifetime:\n",
              "  - Train state-of-the-art models in 3 lines of code.\n",
              "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
              "  - Seamlessly pick the right framework for training, evaluation and production.\n",
              "\n",
              "4. Easily customize a model or an example to your needs:\n",
              "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
              "  - Model internal [SEP]\n",
              "\"\"\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j01PpqxrPpw",
        "outputId": "33082367-7d17-49bf-c6db-4780a0962c44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] This sentence is not [SEP]'\n",
              "'[CLS] is not too long [SEP]'\n",
              "'[CLS] too long but we [SEP]'\n",
              "'[CLS] but we are going [SEP]'\n",
              "'[CLS] are going to split [SEP]'\n",
              "'[CLS] to split it anyway [SEP]'\n",
              "'[CLS] it anyway. [SEP]'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
        "inputs = tokenizer(\n",
        "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh-FoQ_-rPpw",
        "outputId": "ac08984c-b4ec-4e33-bf4a-872859351f52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(inputs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKtNNcGOrPpw",
        "outputId": "11aba4ad-d1ca-4c3b-ef35-6415311adf4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhYNcutarPpw",
        "outputId": "b4754615-7fac-41f9-d4d2-3d4ad2d84915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"This sentence is not too long but we are going to split it anyway.\",\n",
        "    \"This sentence is shorter but will still get split.\",\n",
        "]\n",
        "inputs = tokenizer(\n",
        "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOX2yofgrPpw"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    long_context,\n",
        "    stride=128,\n",
        "    max_length=384,\n",
        "    padding=\"longest\",\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZQBhx94rPpx",
        "outputId": "2b5fe4b0-9cfb-4bee-f84b-036d76051a9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 384])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "offsets = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "inputs = inputs.convert_to_tensors(\"pt\")\n",
        "print(inputs[\"input_ids\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBhxoQ-9rPpx",
        "outputId": "723525a7-7803-47e8-bba7-4bb41e3e4427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 384]) torch.Size([2, 384])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "print(start_logits.shape, end_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRtSWzHPrPpx"
      },
      "outputs": [],
      "source": [
        "sequence_ids = inputs.sequence_ids()\n",
        "# Mask everything apart from the tokens of the context\n",
        "mask = [i != 1 for i in sequence_ids]\n",
        "# Unmask the [CLS] token\n",
        "mask[0] = False\n",
        "# Mask all the [PAD] tokens\n",
        "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
        "\n",
        "start_logits[mask] = -10000\n",
        "end_logits[mask] = -10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWeqTTuvrPpx"
      },
      "outputs": [],
      "source": [
        "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
        "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeeZ6T6MrPpx",
        "outputId": "c608f50b-4132-4a17-fd80-09f13dbfcf0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 18, 0.33867), (173, 184, 0.97149)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "candidates = []\n",
        "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
        "    scores = start_probs[:, None] * end_probs[None, :]\n",
        "    idx = torch.triu(scores).argmax().item()\n",
        "\n",
        "    start_idx = idx // scores.shape[1]\n",
        "    end_idx = idx % scores.shape[1]\n",
        "    score = scores[start_idx, end_idx].item()\n",
        "    candidates.append((start_idx, end_idx, score))\n",
        "\n",
        "print(candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiL-w642rPpx",
        "outputId": "0fc75729-c657-4a68-b086-03768f7bf86d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}\n",
              "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for candidate, offset in zip(candidates, offsets):\n",
        "    start_token, end_token, score = candidate\n",
        "    start_char, _ = offset[start_token]\n",
        "    _, end_char = offset[end_token]\n",
        "    answer = long_context[start_char:end_char]\n",
        "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50OxlJrrrUbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqHmQ8XvrTdP"
      },
      "source": [
        "# Normalization and pre-tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UlA3ZKrTdQ"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N_tSHmrrTdQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOTVxVPFrTdQ",
        "outputId": "dc65ba6c-8f2d-42a8-bae6-873b234114e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'tokenizers.Tokenizer'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(type(tokenizer.backend_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3lArEQ-rTdQ",
        "outputId": "ec803b7c-d39a-4348-9f62-90131d7ac07c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tknRNIKGrTdR",
        "outputId": "a6324d3b-b45e-4443-c14e-8ff72fcbca45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ujgQM8FrTdR",
        "outputId": "48142fa5-c518-4fe2-ce2e-0e62ad5e9bcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),\n",
              " ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yRCigtnrTdR",
        "outputId": "4867ee8f-a776-4b3f-9abe-a8832d54aea8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOZ35qPqrYHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CBMr7OmrWRQ"
      },
      "source": [
        "# Byte-Pair Encoding tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySQ8wgtKrWRR"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp7LCix3rWRR"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTxoC8zNrWRS"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GETNNRR9rWRS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mlw9SCkrWRS",
        "outputId": "9f691fb8-e903-49a1-d73a-7eb0b7d220d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(int, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1,\n",
              "    'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1,\n",
              "    'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1,\n",
              "    'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSdJV_nBrWRS",
        "outputId": "e9c41d3d-3bb4-482c-e36f-b1a4bf41b12b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',\n",
              "  't', 'u', 'v', 'w', 'y', 'z', 'Ġ']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGFmsJg4rWRS"
      },
      "outputs": [],
      "source": [
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiwmxriyrWRS"
      },
      "outputs": [],
      "source": [
        "splits = {word: [c for c in word] for word in word_freqs.keys()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72cmJhlIrWRS"
      },
      "outputs": [],
      "source": [
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeqyYfWfrWRS",
        "outputId": "f98ad43e-e68f-4943-b89d-a331afb4f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('T', 'h'): 3\n",
              "('h', 'i'): 3\n",
              "('i', 's'): 5\n",
              "('Ġ', 'i'): 2\n",
              "('Ġ', 't'): 7\n",
              "('t', 'h'): 3"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXOCxxJLrWRS",
        "outputId": "9f6bed02-7ec9-4bf3-bc94-737f58ea2828"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Ġ', 't') 7"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zamQG0QYrWRT"
      },
      "outputs": [],
      "source": [
        "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
        "vocab.append(\"Ġt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ecb9osqrWRT"
      },
      "outputs": [],
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsMzq6A5rWRT",
        "outputId": "7a537dfc-9736-49d6-a173-a8e4a50bf415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
        "print(splits[\"Ġtrained\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZVGNlR8rWRT"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwSmWr1GrWRT",
        "outputId": "a2c92cbc-b0d2-47cf-92b1-b2a64f6133cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en',\n",
              " ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok',\n",
              " ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe',\n",
              " ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNGtLe97rWRT",
        "outputId": "79748c61-a7a7-4566-ee2b-44beb7c4ffeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',\n",
              " 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se',\n",
              " 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrzTtXwIrWRT"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQdwtQAlrWRT",
        "outputId": "52557a44-3683-4f84-d009-a55c8fa35f1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize(\"This is not a token.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eI1qCbYQrbKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoRPkt1rZ7H"
      },
      "source": [
        "# WordPiece tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0ELS0xirZ7I"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1yOm9YJrZ7I"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jjjd-tjrZ7I"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRNVD1A1rZ7I"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KQ2TyX4rZ7I",
        "outputId": "f7e0d02d-8167-4283-e7c2-f452f3b2c5e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(\n",
              "    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,\n",
              "    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,\n",
              "    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,\n",
              "    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqIQZ_FYrZ7I",
        "outputId": "1bbac6d1-7dd1-416d-8236-e3e0ba9cca8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',\n",
              " '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',\n",
              " 'w', 'y']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "alphabet\n",
        "\n",
        "print(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyFqmnATrZ7J"
      },
      "outputs": [],
      "source": [
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIO0Cm3-rZ7J"
      },
      "outputs": [],
      "source": [
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ildYQZbGrZ7J"
      },
      "outputs": [],
      "source": [
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hNTFPfvrZ7J",
        "outputId": "5910b57c-51ec-4740-f03e-bcc2a775ff7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('T', '##h'): 0.125\n",
              "('##h', '##i'): 0.03409090909090909\n",
              "('##i', '##s'): 0.02727272727272727\n",
              "('i', '##s'): 0.1\n",
              "('t', '##h'): 0.03571428571428571\n",
              "('##h', '##e'): 0.011904761904761904"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxShU6K-rZ7J",
        "outputId": "a75a232a-a29f-4813-cb2e-b711f690fb1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('a', '##b') 0.2"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "print(best_pair, max_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pG1-FverZ7J"
      },
      "outputs": [],
      "source": [
        "vocab.append(\"ab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTzxGzacrZ7J"
      },
      "outputs": [],
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy7C5payrZ7J",
        "outputId": "d8c85986-aa96-47d7-a115-af72c7d664de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_tbksByrZ7J"
      },
      "outputs": [],
      "source": [
        "vocab_size = 70\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = (\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    vocab.append(new_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74lBaCQsrZ7J",
        "outputId": "fe787bbd-0b66-437b-a7bc-023223a34664"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',\n",
              " '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',\n",
              " 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',\n",
              " 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',\n",
              " '##ut']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp3Cch0trZ7J"
      },
      "outputs": [],
      "source": [
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAioThWerZ7J",
        "outputId": "f39fa16b-53a3-4b0e-b872-2df73d2fc4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hugg', '##i', '##n', '##g']\n",
              "['[UNK]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNCMC-cgrZ7J"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgQYCnc9rZ7J",
        "outputId": "c5d17faf-73eb-4b99-d0a8-452bc6ffd1a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',\n",
              " '##e', '[UNK]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNQjkOpWrgEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PC-EmV2rdvk"
      },
      "source": [
        "# Unigram tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJyHg-sNrdvl"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvQEJnO0rdvl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA4gMMn8rdvm"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idfQDnGIrdvn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12c36bl-rdvn"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clRWjtmfrdvn",
        "outputId": "73011207-bfd3-480e-dc5b-56a04d9971db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('▁t', 7), ('is', 5), ('er', 5), ('▁a', 5), ('▁to', 4), ('to', 4), ('en', 4), ('▁T', 3), ('▁Th', 3), ('▁Thi', 3)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_freqs = defaultdict(int)\n",
        "subwords_freqs = defaultdict(int)\n",
        "for word, freq in word_freqs.items():\n",
        "    for i in range(len(word)):\n",
        "        char_freqs[word[i]] += freq\n",
        "        # Loop through the subwords of length at least 2\n",
        "        for j in range(i + 2, len(word) + 1):\n",
        "            subwords_freqs[word[i:j]] += freq\n",
        "\n",
        "# Sort subwords by frequency\n",
        "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_subwords[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnDON0O_rdvo"
      },
      "outputs": [],
      "source": [
        "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
        "token_freqs = {token: freq for token, freq in token_freqs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnq0gxJPrdvo"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "\n",
        "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoa1B0p4rdvo"
      },
      "outputs": [],
      "source": [
        "def encode_word(word, model):\n",
        "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
        "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
        "    ]\n",
        "    for start_idx in range(len(word)):\n",
        "        # This should be properly filled by the previous steps of the loop\n",
        "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
        "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
        "            token = word[start_idx:end_idx]\n",
        "            if token in model and best_score_at_start is not None:\n",
        "                score = model[token] + best_score_at_start\n",
        "                # If we have found a better segmentation ending at end_idx, we update\n",
        "                if (\n",
        "                    best_segmentations[end_idx][\"score\"] is None\n",
        "                    or best_segmentations[end_idx][\"score\"] > score\n",
        "                ):\n",
        "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
        "\n",
        "    segmentation = best_segmentations[-1]\n",
        "    if segmentation[\"score\"] is None:\n",
        "        # We did not find a tokenization of the word -> unknown\n",
        "        return [\"<unk>\"], None\n",
        "\n",
        "    score = segmentation[\"score\"]\n",
        "    start = segmentation[\"start\"]\n",
        "    end = len(word)\n",
        "    tokens = []\n",
        "    while start != 0:\n",
        "        tokens.insert(0, word[start:end])\n",
        "        next_start = best_segmentations[start][\"start\"]\n",
        "        end = start\n",
        "        start = next_start\n",
        "    tokens.insert(0, word[start:end])\n",
        "    return tokens, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NyGZ8N2rdvo",
        "outputId": "1ce99b8b-0bbc-4666-bb3a-38a97740d34f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
              "(['This'], 6.288267030694535)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(encode_word(\"Hopefully\", model))\n",
        "print(encode_word(\"This\", model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsLnUVJ5rdvp"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model):\n",
        "    loss = 0\n",
        "    for word, freq in word_freqs.items():\n",
        "        _, word_loss = encode_word(word, model)\n",
        "        loss += freq * word_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccHPElsBrdvp",
        "outputId": "a96653b3-80d7-408d-a07e-62e68e18dd51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "413.10377642940875"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jleUBJ_2rdvp"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def compute_scores(model):\n",
        "    scores = {}\n",
        "    model_loss = compute_loss(model)\n",
        "    for token, score in model.items():\n",
        "        # We always keep tokens of length 1\n",
        "        if len(token) == 1:\n",
        "            continue\n",
        "        model_without_token = copy.deepcopy(model)\n",
        "        _ = model_without_token.pop(token)\n",
        "        scores[token] = compute_loss(model_without_token) - model_loss\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuVyc5Bdrdvp",
        "outputId": "0489f1c5-84d2-48c6-a9c6-395497931a90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.376412403623874\n",
              "0.0"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = compute_scores(model)\n",
        "print(scores[\"ll\"])\n",
        "print(scores[\"his\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "838qY02qrdvp"
      },
      "outputs": [],
      "source": [
        "percent_to_remove = 0.1\n",
        "while len(model) > 100:\n",
        "    scores = compute_scores(model)\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
        "    # Remove percent_to_remove tokens with the lowest scores.\n",
        "    for i in range(int(len(model) * percent_to_remove)):\n",
        "        _ = token_freqs.pop(sorted_scores[i][0])\n",
        "\n",
        "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64S9Lnxnrdvq",
        "outputId": "c50e4e81-2184-46f0-921a-fddd8f94e5f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁', 'c', 'ou', 'r', 's', 'e', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(text, model):\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
        "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "tokenize(\"This is the Hugging Face course.\", model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQb0I__jrjSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41VjKJqjriCW"
      },
      "source": [
        "# Building a tokenizer, block by block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0cV0CGKriCX"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PonS2EQ7riCX"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEiX1DIRriCX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw94-7GuriCX"
      },
      "outputs": [],
      "source": [
        "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(dataset)):\n",
        "        f.write(dataset[i][\"text\"] + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcvkYzBMriCX"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi012vsIriCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvrdIb3FriCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxdt97A5riCY",
        "outputId": "4d5243f0-3479-467d-84e8-24adb2dae708"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hello how are u?"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0CXyk-4riCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-07u6GCriCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "occrYW-eriCY",
        "outputId": "57b306d0-a533-4e52-e245-af44413494c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),\n",
              " ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5qVBCcqriCY",
        "outputId": "6ed89cb7-abdf-44a6-a3b5-9c693374a5b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"Let's\", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aaOvUr-riCY",
        "outputId": "776970df-8d08-4ac8-b3a6-c08b88b6bd23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),\n",
              " ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76l4tSDmriCY"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv8GCwaKriCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dw8RdIxriCY"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pTeYBJOriCY",
        "outputId": "7590873d-e540-4ebc-9409-c45a3c52aaa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In3QeB1uriCY",
        "outputId": "d3fc4dae-41ed-4635-a044-88ddca61a56a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y1_FQjTriCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZz8I3_ZriCZ",
        "outputId": "5cae5e65-d50f-4d42-e15d-6d20db4b0ea8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngNEe9yAriCZ",
        "outputId": "5c311909-3a1b-42d9-dda0-2365657447a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8POS0kWriCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVQb2H6driCZ",
        "outputId": "29cab769-e166-4f7f-defb-2107049c8dcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"let's test this tokenizer... on a pair of sentences.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0ED1Na5riCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n4peU9nriCZ"
      },
      "outputs": [],
      "source": [
        "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ElzOU1XriCZ"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHUiXClSriCZ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOCu0NZcriCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q-w99aRriCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX-rAmG4riCZ",
        "outputId": "6d27b1a8-4de4-4a84-e643-ffdb4e33c15b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'s\", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)), ('!', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bua2UGsPriCZ"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHCu9JJRriCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.BPE()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw9jkSE1riCZ",
        "outputId": "4addd168-3581-4a49-badf-3e2f03b8a33f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfEHro98riCZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAyK83qJriCZ",
        "outputId": "197f707d-5bbc-4241-e574-e4a8e89ca7a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' test'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLzXKlnBriCa"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xJJhb8yriCa",
        "outputId": "e9fb2e6a-b08f-4268-9b38-3ce6c27ea5fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Let's test this tokenizer.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9hzOZd2riCa"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBie1XD7riCd"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEgNM3FwriCd"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr_DhucBriCd"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmZj9uizriCd"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLFImuPqriCd",
        "outputId": "dbcc3994-f750-4189-9de4-54ce919d300f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"▁Let's\", (0, 5)), ('▁test', (5, 10)), ('▁the', (10, 14)), ('▁pre-tokenizer!', (14, 29))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to3wFdBIriCd"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HL7L_KiriCd"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7cINAk_riCd",
        "outputId": "fea13827-65da-4e70-8241-d24ae5d2f716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZwXVqN6riCe",
        "outputId": "de39f23f-a84d-4832-8671-e08d8cc70973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0 1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOJYBcjHriCe"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fZSUteiriCe",
        "outputId": "3744742b-6ca3-4882-d770-0e543f6fed6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', \n",
              "  '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj2u_EFSriCe"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jozTVtVgriCe"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    cls_token=\"<cls>\",\n",
        "    sep_token=\"<sep>\",\n",
        "    mask_token=\"<mask>\",\n",
        "    padding_side=\"left\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmn6cAcfriCe"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
      ]
    }
  ]
}